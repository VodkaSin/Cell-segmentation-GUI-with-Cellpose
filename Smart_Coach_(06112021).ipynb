{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Smart Coach (CAA 201021 1700hrs).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "reAF-7dlY4L6",
        "F7dqLZ4wY6zi",
        "kgaabAaEe9rh",
        "9_y6ZIrrhZ-y",
        "4AJc_GkqjUIv",
        "5MPLOMk9ZjCe",
        "DMePgRAEl92Z",
        "cUjO-IpHmHVI",
        "-RIDHvy0nrqE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VodkaSin/Cell-segmentation-GUI-with-Cellpose/blob/main/Smart_Coach_(06112021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlFi_G71ktDQ"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_IbQPkPVSxR"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqABktRocVos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de0db5e-9b2e-4804-c8b7-c86df7a10d42"
      },
      "source": [
        "!pip install -q tensorflow==2.6.0 tensorflow-gpu==2.6.0\n",
        "!pip install -q gTTS\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 458.3 MB 12 kB/s \n",
            "\u001b[?25h  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reAF-7dlY4L6"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZZtn6HacbN7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow_docs.vis import embed \n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.collections import LineCollection \n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio \n",
        "import PIL\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Javascript, Image, HTML, Audio # When are all these used?\n",
        "from gtts import gTTS\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "# Data access (privacy)\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dqLZ4wY6zi"
      },
      "source": [
        "## Visualization helper functions\n",
        "\n",
        "1. Streaming APIs\n",
        "2. Recording and display videos\n",
        "3. Smart cropping\n",
        "4. Frame rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmrETs3ceMR"
      },
      "source": [
        "### Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyqTz_0Mccci"
      },
      "source": [
        "# JavaScript to create video stream from webcam\n",
        "def start_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span></span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      video.style.cssText = \"-moz-transform: scale(-1, 1); \\\n",
        "-webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); \\\n",
        "transform: scale(-1, 1); filter: FlipH;\";\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 192; //video.videoWidth;\n",
        "      captureCanvas.height = 192; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH5UVooPwhde"
      },
      "source": [
        "# Capture frame in bytes from streaming\n",
        "def take_frame(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBZCxBfcn9w"
      },
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\" Used in main function to convert image types\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BRG image (mirror flipped)\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply['img'].split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  img = cv2.flip(img,1)\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJcIDVNIct7C"
      },
      "source": [
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\" Used in the main function to convert numpy overlay onto the frame\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgaabAaEe9rh"
      },
      "source": [
        "### Recording and displaying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2DwJl_0gE1A"
      },
      "source": [
        "def record_video(video_path):\n",
        "  \"\"\" Called in the main function as an option to record the exercise video for later reference\n",
        "  Params:\n",
        "          video_path: a path name for storing the video\n",
        "  \"\"\"\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
        "    \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(video_path,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{video_path}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BIG70Xe9RZ"
      },
      "source": [
        "def show_video(video_path, video_width = 600):\n",
        "  \"\"\" Called when the user wants to review their workout recording and pose estimation\n",
        "  Params:\n",
        "          video_path: local\n",
        "          video_width: default = 600\n",
        "  Returns:\n",
        "          Display the video in cell\n",
        "  \"\"\"\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_y6ZIrrhZ-y"
      },
      "source": [
        "### Cropping algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnyKD2UOhqZs"
      },
      "source": [
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region, provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWcfXkPeiD8m"
      },
      "source": [
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEGB0gMSiATH"
      },
      "source": [
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsauaf9iJL3"
      },
      "source": [
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNMcvxfniLAm"
      },
      "source": [
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evfuBCWZiOGX"
      },
      "source": [
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJc_GkqjUIv"
      },
      "source": [
        "### Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLOcHtuAHrd"
      },
      "source": [
        "def draw_keypoints(frame, keypoints, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    points = []\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            array = cv2.circle(array, (int(kx), int(ky)), 3, (0,255,0), -1)\n",
        "\n",
        "def draw_connections(frame, keypoints, edges, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    \n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "        \n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
        "            array = cv2.line(array, (int(x1), int(y1)), (int(x2), int(y2)), (255,0,0), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww-ZBCQOxUBr"
      },
      "source": [
        "def draw_text(array, org, text, color):\n",
        "  array = cv2.putText(array, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1, cv2.LINE_AA )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MPLOMk9ZjCe"
      },
      "source": [
        "## Model algorithm\n",
        "\n",
        "1. Libraries of definitons (joints and angles), \n",
        "2. Model loading\n",
        "3. Angle calculation algorithm\n",
        "4. Squat counter logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMePgRAEl92Z"
      },
      "source": [
        "### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0CrmuIQi_BY"
      },
      "source": [
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "\n",
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "\n",
        "# Note that the image is flipped, here the keys refer to the user's pov\n",
        "THREE_JOINT_INDEX = {\n",
        "    'right_shoulder (H)': (11,5,7),\n",
        "    'right_shoulder (S)': (7,5,6),\n",
        "    'left_shoulder (H)': (12,6,8),\n",
        "    'left_shoulder (S)': (8,6,5),\n",
        "    'right_elbow':(5,7,9),\n",
        "    'left_elbow':(6,8,10),\n",
        "    'right_torso':(5,11,13),\n",
        "    'left_torso':(6,12,14),\n",
        "    'right_knee':(11,13,15),\n",
        "    'left_knee':(12,14,16),\n",
        "    'right_torso turn':(6,5,11),\n",
        "    'left_torso_turn':(5,6,12)\n",
        "}\n",
        "FOUR_JOINT_INDEX ={\n",
        "    'shoulders_hips':(5,6,11,12),\n",
        "    'btw_legs':(11,13,12,14)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjO-IpHmHVI"
      },
      "source": [
        "### Model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrDji3XgmJ6n"
      },
      "source": [
        "# Grab from TF hub and initialize interpreter\n",
        "!wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3?lite-format=tflite\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gJ9XxwmRhA"
      },
      "source": [
        "# Interpreter\n",
        "def movenet(frame):\n",
        "  \"\"\" Runs detection on an input frame\n",
        "  Params:\n",
        "          frame: 192x192 opencv BRG\n",
        "  Returns:\n",
        "          keypoints_with_scores: [index, location, confidence]\n",
        "  \"\"\"  \n",
        "  # Reshape iamge to 192x192x3\n",
        "  img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 192,192)\n",
        "  input_image = tf.cast(img, dtype=tf.float32) # Specify data type\n",
        "\n",
        "  # Setup input and output \n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # Make predictions \n",
        "  interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "  interpreter.invoke()\n",
        "  keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Fq9-q_ntg8"
      },
      "source": [
        "### Squat counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D39YEsmkZK3m"
      },
      "source": [
        "def find_side_squat(keypoints_with_scores, threshold=0.2):\n",
        "  '''1. Extract the joints needed: 5,6 (shoulders), 11,12 (hips), 13,14 (knees), 15,16 (ankles)\n",
        "     2. Compare the confidence for both sides: use only the side with higher confidence and if any \n",
        "        angle in the confidence side < threshold, output key 'Bad detection'= False\n",
        "     3. Ouput side [keypoints_with_scores(shoulder, hip, knee, ankle)]\n",
        "     Note: here left and right is the opposite of the user's left and right due to the mirroring effect.'''\n",
        "  left = [keypoints_with_scores[0][0][5],keypoints_with_scores[0][0][11],keypoints_with_scores[0][0][13],keypoints_with_scores[0][0][15]]\n",
        "  right = [keypoints_with_scores[0][0][6],keypoints_with_scores[0][0][12],keypoints_with_scores[0][0][14],keypoints_with_scores[0][0][16]]\n",
        "  left_confidence = [left[i][2] for i in range (4)]\n",
        "  right_confidence = [right[i][2] for i in range (4)]\n",
        "  if sum(left_confidence) > sum(right_confidence):\n",
        "    side = np.asarray(left)\n",
        "    confidence = left_confidence\n",
        "  else:\n",
        "    side = np.asarray(right)\n",
        "    confidence = right_confidence\n",
        "  for i in range(3):\n",
        "    if confidence[i]<threshold:\n",
        "      return None, False\n",
        "  return np.delete(side,2,1), True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOW7RdDigcpN"
      },
      "source": [
        "def squat_angles(side):\n",
        "  an1 = getAngle(side[0],side[1],side[2])\n",
        "  an2 = getAngle(side[1],side[2],side[3])\n",
        "  torso = an1 if an1 < 180 else 360-an1\n",
        "  knee = an2 if an2 < 180 else 360-an2\n",
        "  return torso, knee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auhk2YdGFK69"
      },
      "source": [
        "def squat_count(torso, knee):\n",
        "  '''Default: \n",
        "    1. knee flexion below 100 is considered ok, the user should aim for squatting\n",
        "           down deeper, below 60 is excellent mobility.\n",
        "    2. hip flexion is counted when angle smaller than 90 but should be larger than 70\n",
        "    3. getting up, the knee should be 180 in ideal case, 170 is considered OK, same for hip flexion\n",
        "    \n",
        "    Parameters:\n",
        "    angle: [hip flexion left, hip flexion right, knee flexion left, knee flexion right] list of degree\n",
        "    stage = string, indicator for the counter, if 'down' counter+1'''\n",
        "  if torso < 125:\n",
        "    stage = 'down'\n",
        "    score = 1\n",
        "    if knee < 95 and torso > 45:\n",
        "      score = 2\n",
        "  else:\n",
        "    if torso <160:\n",
        "      stage = 'down'\n",
        "    else: \n",
        "      stage = 'up'\n",
        "    score = -1\n",
        "  return stage, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RIDHvy0nrqE"
      },
      "source": [
        "### Angle calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTbIv83Yn46D"
      },
      "source": [
        "def getAngle(a, b, c):\n",
        "    ang = abs(math.degrees(math.atan2(b[1]-c[1], b[0]-c[0]) - math.atan2(b[1]-a[1], b[0]-a[0])))\n",
        "    return ang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S5loWastc9e"
      },
      "source": [
        "def getAngle2(a,b,c,d):\n",
        "  # For 4 point inputs (two crossing lines)\n",
        "  dx = c[1]-a[1]\n",
        "  dy = c[0]-a[0]\n",
        "  return getAngle(b,a,(d[0]-dx,d[1]-dy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QsayVAuP8v"
      },
      "source": [
        "def find_angle_3(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in THREE_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle(a[0],a[1],a[2])\n",
        "  return angles   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1udy1FkuRsP"
      },
      "source": [
        "def find_angle_4(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in FOUR_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle2(a[0],a[1],a[2],a[3])\n",
        "  return angles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfdE_-28hf_V"
      },
      "source": [
        "## Main inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_zpkxEkhjQ1"
      },
      "source": [
        "def smart_coach():\n",
        "\n",
        "  goal = input('Enter your target score: ')\n",
        "  start_stream()\n",
        "  label_html = 'Smart Coach'  #label\n",
        "  img_data = ''\n",
        "  max_score = 0\n",
        "  total_score = 0\n",
        "  total_good = 0\n",
        "  total_excellent = 0\n",
        "  total_bad = 0\n",
        "  combo_bad = 0\n",
        "  combo_excellent = 0\n",
        "  store = []\n",
        "  times = np.empty(1)\n",
        "  forms = []\n",
        "  bad_torsos = []\n",
        "  bad_knees = []\n",
        "  low_torso = 0\n",
        "\n",
        "\n",
        "  '''image_height, image_width = 192, 192\n",
        "  crop_region = init_crop_region(image_height, image_width)'''\n",
        "  while True:\n",
        "      js_reply = take_frame(label_html, img_data)\n",
        "      if not js_reply:\n",
        "        print('Streaming has been interupted')\n",
        "        break\n",
        "      start = time.time()\n",
        "      # Process Javascript Object (frame) to OpenCV BRG\n",
        "      frame = js_to_image(js_reply)\n",
        "      # Initialize render array\n",
        "      img_array= np.zeros([192,192,4], dtype=np.uint8)\n",
        "      # Run the interpreter on the BRG image\n",
        "      keypoints_with_scores = movenet(frame)\n",
        "      side, flag = find_side_squat(keypoints_with_scores)\n",
        "      if flag == False:\n",
        "        draw_text(img_array,(10,100),'Please adjust your position', (255,0,0))\n",
        "      else:\n",
        "        torso, knee = squat_angles(side)\n",
        "        if torso<45:\n",
        "          low_torso += 1\n",
        "          if low_torso == 1:\n",
        "            hunch_form = frame.copy()\n",
        "            draw_text(hunch_form,(10,35),'Hunched back: '+str(int(low_torso)), (255,255,0))\n",
        "            forms.append(hunch_form)\n",
        "          draw_text(img_array,(10,100),'Please straighten your back', (255,255,0))\n",
        "        stage, score = squat_count(torso, knee)\n",
        "        store.append([torso,knee,stage,score])\n",
        "        #draw_text(img_array,(100,120),'torso ' + str(torso), (255,0,0))\n",
        "        #draw_text(img_array,(100,140),'knee ' + str(knee), (255,0,0))\n",
        "        # Only count the highest score in one rep and avoid double counting\n",
        "        if stage =='down':\n",
        "          if score == 2 and score > max_score: \n",
        "            max_score = score\n",
        "            total_score += max_score\n",
        "            total_excellent += 1\n",
        "            combo_bad = 0\n",
        "            combo_excellent += 1\n",
        "            draw_text(img_array,(50,50),'Excellent x'+str(combo_excellent), (255,0,127))\n",
        "            if total_excellent == 1:\n",
        "              excellent_form = frame.copy()\n",
        "              draw_text(excellent_form,(10,35),'Excellent! ', (255,0,127))\n",
        "              forms.append(excellent_form)\n",
        "            if combo_excellent > 3:\n",
        "              total_score += 1\n",
        "          if score == 1 and max_score < 2:\n",
        "            max_score = score\n",
        "          if score == -1 and max_score == 0:\n",
        "            max_score = score\n",
        "            bad_form = frame.copy()\n",
        "            bad_keypoints = keypoints_with_scores\n",
        "            bad_torso = torso\n",
        "            bad_knee = knee\n",
        "        if stage == 'up':\n",
        "          if max_score == 1:\n",
        "            total_score += max_score\n",
        "            total_good += 1\n",
        "            combo_bad = 0\n",
        "            combo_excellent = 0\n",
        "            draw_text(img_array,(50,50),'Good!', (102,178,255))\n",
        "            if total_good == 1:\n",
        "              good_form = frame.copy()\n",
        "              draw_text(good_form,(10,35),'Can squat down more', (255,0,127))\n",
        "              forms.append(good_form)\n",
        "          if max_score == -1:\n",
        "            total_bad += 1\n",
        "            combo_bad +=1\n",
        "            combo_excellent = 0\n",
        "            if total_bad < 4:\n",
        "              draw_text(bad_form,(10,20),'Miss x '+str(total_bad), (255,255,0))\n",
        "              draw_keypoints(frame, bad_keypoints, bad_form, 0.2)\n",
        "              draw_connections(frame, bad_keypoints, EDGES, bad_form, 0.2)\n",
        "              draw_text(bad_form,(10,35),'Torso: '+str(int(bad_torso)), (255,255,0))\n",
        "              draw_text(bad_form,(10,50),'Knee: '+str(int(bad_knee)), (255,255,0))\n",
        "            draw_text(img_array,(50,50),'Miss x '+str(combo_bad), (0,0,255))\n",
        "            bad_torsos.append(int(bad_torso))\n",
        "            bad_knees.append(int(bad_knee))\n",
        "            forms.append(bad_form)\n",
        "          score = 0\n",
        "          max_score = 0\n",
        "      if combo_bad > 3:\n",
        "        break\n",
        "      if total_score == goal:\n",
        "        draw_text(img_array,(10,100),'Goal achieved! Take a break!', (0,255,0))\n",
        "        break\n",
        "      # Rendering the keypoints and edges: add to img_array and convert to JS, uncomment if you want to see the pose estimation\n",
        "      draw_text(img_array,(10,20),'Total score: '+str(total_score), (102,255,102))\n",
        "      draw_text(img_array,(10,35),'Total count: '+str(total_good+total_excellent), (255,255,0))\n",
        "      draw_text(img_array,(10,185),'Total miss: '+str(total_bad), (0,0,255))\n",
        "      img_array[:,:,3] = (img_array.max(axis = 2)>0).astype(int)*255\n",
        "      img_data = bbox_to_bytes(img_array)\n",
        "      stop = time.time()\n",
        "      duration = stop-start\n",
        "      times = np.append(times, duration)\n",
        "      \n",
        "  if total_bad>5 or combo_bad>3:\n",
        "    tts = gTTS('Game over! See feedback below!')\n",
        "  else:\n",
        "    tts = gTTS('Workout completed, congratulations! See feedback below!')\n",
        "    \n",
        "  tts.save('1.wav')\n",
        "  sound_file = '1.wav'\n",
        "  Audio(sound_file, autoplay=True)\n",
        "\n",
        "  print('---------WORKOUT SUMMARY---------'+'\\n')\n",
        "  print('Total score     |         ',total_score)\n",
        "  print('Total reps      |         ', total_good+total_excellent)\n",
        "  print('Excellent reps  |         ',total_excellent)\n",
        "  print('Good reps       |         ',total_good)\n",
        "  print('Missed reps     |         ', total_bad,'\\n')\n",
        "\n",
        "  if total_bad>2:\n",
        "    # Show screenshots of the person's bad squats x3\n",
        "    torsos = round(sum(bad_torsos)/len(bad_torsos),2)\n",
        "    knees = round(sum(bad_knees)/len(bad_knees),2)\n",
        "    print('-----------FEEDBACK-----------'+'\\n')\n",
        "    print('1. In the missed reps, your average hip flexion is',str(torsos),'degrees, aim for 70 to 95 degrees')\n",
        "    if knees > 95:\n",
        "      print('2. Your knees seem too stiff, aim for 60 to 95 degrees')\n",
        "    if low_torso != 0: \n",
        "      print('3. We also noticed that you might have been hunching your back during the excercise for',str(low_torso),'times, try to avoid that!')\n",
        "    print('\\n'+'Don\\'t worry, just try again, you will get better!')\n",
        "    \n",
        "  if input('\\n'+'Check your form? (y/n): ')=='y':\n",
        "      for photo in forms:\n",
        "          cv2_imshow(photo)      \n",
        "  else:\n",
        "    print('Well done!')\n",
        "  return   total_score, total_good, total_excellent, total_bad, times, store"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kD1IYcwa7mV"
      },
      "source": [
        "# Smart Coach\n",
        "\n",
        "1. Streaming of mirrored web cam input with pose estimation rendered on frame\n",
        "2. Print output for immediate reading\n",
        "3. Text output for record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1e9VOFyaFMJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "2db9220d-ad6e-4706-b1bf-39e6493b94dc"
      },
      "source": [
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Singapore /etc/localtime\n",
        "start_time = !date\n",
        "start_time = int(start_time[0][11:-9].replace(':',''))\n",
        "\n",
        "total_score, total_good, total_excellent, total_bad, times, store = smart_coach()\n",
        "\n",
        "end_time = !date\n",
        "end_time = int(end_time[0][11:-9].replace(':',''))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your target score: 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span></span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      video.style.cssText = \"-moz-transform: scale(-1, 1); -webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); transform: scale(-1, 1); filter: FlipH;\";\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 192; //video.videoWidth;\n",
              "      captureCanvas.height = 192; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming has been interupted\n",
            "---------WORKOUT SUMMARY---------\n",
            "\n",
            "Total score     |          0\n",
            "Total reps      |          0\n",
            "Excellent reps  |          0\n",
            "Good reps       |          0\n",
            "Missed reps     |          0 \n",
            "\n",
            "\n",
            "Check your form? (y/n): n\n",
            "Well done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ut4e9CZmBiY"
      },
      "source": [
        "# Workout Efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOB5WEqf4ZDh"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "lgIIelHl_x-l",
        "outputId": "fe1ab1d1-0dd1-42be-fed8-94198b7c5d7f"
      },
      "source": [
        "worksheet = gc.open('Sensor').sheet1\n",
        "spr=0.25\n",
        "rows = worksheet.get_all_values()\n",
        "length = len(rows)\n",
        "datetimes = [rows[i][0] for i in range (1,length)]\n",
        "start_index = 1\n",
        "end_index = 2\n",
        "print(start_time)\n",
        "print(end_time)\n",
        "for t in range(length-1):\n",
        "  sheet_time = int(datetimes[t][-8:].replace(':',''))\n",
        "  if sheet_time > start_time:\n",
        "    start_index = t\n",
        "    print(start_index)\n",
        "    print('Start:', sheet_time)\n",
        "    break\n",
        "for k in range (start_index,length-1):\n",
        "  sheet_time2 = int(datetimes[k][-8:].replace(':',''))\n",
        "  if sheet_time2 > end_time:\n",
        "    end_index = k\n",
        "    print(end_index)\n",
        "    print('End:', sheet_time2)\n",
        "    break\n",
        "heart_rates = [int(rows[i][1]) for i in range(start_index,end_index)]\n",
        "blood_ox = [int(rows[i][2]) for i in range(start_index,end_index)]\n",
        "times = [i/spr for i in range(end_index-start_index)]\n",
        "plt.figure() \n",
        "plt.subplot(2, 1, 1) # (rows, columns, panel number)\n",
        "plt.plot(times,heart_rates, color='orange')\n",
        "plt.ylabel('Heart rate (bpm)')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(times, blood_ox, color='blue')\n",
        "plt.ylabel('VO2 (%)')\n",
        "plt.xlabel('time (s)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221522\n",
            "221534\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e6bcc9bf0e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0msheet_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msheet_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5f8fuE5HC-F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "a41da36e-6e90-4b8a-f48f-5466332b1781"
      },
      "source": [
        "MHR = input('Enter your maximum heart rate (or ''\\'n''\\' if unknown): ')\n",
        "if MHR == 'n':\n",
        "  age = int(input('Enter your age: '))\n",
        "  MHR = 220-age\n",
        "else:\n",
        "  MHR=int(MHR)\n",
        "print(MHR)\n",
        "bins = [0.3*MHR,0.6*MHR,0.7*MHR,0.8*MHR,0.9*MHR,1*MHR]\n",
        "labels = ['Zone 1','Zone 2','Zone 3','Zone 4','Zone 5']\n",
        "colors = ['b','g','y','m','r']\n",
        "handles = [Rectangle((0,0),1,1,color=c) for c in colors]\n",
        "plt.figure()\n",
        "N, bins, patches = plt.hist(heart_rates, bins)\n",
        "count, _ = np.histogram(heart_rates, bins)\n",
        "for i in range (5):\n",
        "  color = colors[i]\n",
        "  patches[i].set_facecolor(color)\n",
        "  plt.text(bins[i]+2, N[i], str(count[i]*spr)+'s', fontsize=14)\n",
        "\n",
        "plt.legend(handles, labels)\n",
        "plt.xlabel(\"Heart rate zones\",fontsize=16)  \n",
        "plt.ylabel(\"Duration (s)\",fontsize=16)\n",
        "plt.xticks(fontsize=14)  \n",
        "plt.yticks(fontsize=14)\n",
        "def scale_time(x,*args):\n",
        "  x = float(x)*spr\n",
        "  return x\n",
        "ax = plt.gca()       \n",
        "ax.yaxis.set_major_formatter(mtick.FuncFormatter(scale_time))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your maximum heart rate (or 'n' if unknown): 198\n",
            "198\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4458b62539b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Zone 1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheart_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4458b62539b2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Zone 1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Zone 5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheart_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Rectangle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-yP-1e8oMoo"
      },
      "source": [
        "### Calories Calculator\n",
        "\n",
        "Female: Calories/min = (-59.3954 + (0.274 * Age) + (0.103 * Weight) + (0.380 * VO2max) + (0.450 * Heart Rate)) / 4.184\n",
        "\n",
        "\n",
        "Male: Calories/min = (-95.7735 + (0.271 * Age) + (0.394 * Weight) + (0.404 * VO2max) + (0.634 * Heart Rate))/4.184"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT71pMsVoMMk"
      },
      "source": [
        "def calories (heart_rates, blood_ox, count, spr):\n",
        "  bpm = sum(heart_rates)\n",
        "  VO2 = max(blood_ox)\n",
        "  fat = [85,85,50,15,10]\n",
        "  protein = [5,5,0,0,0]\n",
        "  carb = [10,10,50,85,90]\n",
        "  fat_percent = []\n",
        "  pro_percent = []\n",
        "  car_percent = []\n",
        "  total = len(count)\n",
        "  if age == None:\n",
        "    age = int(input('Age: '))\n",
        "  gender = input('Gender (F/M): ').lower()\n",
        "  weight = int(input('Weight: '))\n",
        "  if gender == 'f':\n",
        "    calories =  (-59.3954+(0.274*age)+(0.103*weight)+(0.380*VO2)+(0.450*bpm))/(251.04*spr)\n",
        "  elif gender == 'm':\n",
        "    calories =  (-95.7735+(0.271*age)+(0.394*weight)+(0.404*VO2)+(0.634*bpm))/(251.04*spr)\n",
        "\n",
        "for num1, num2, num3, num in zip(fat, protein, carb, count):\n",
        "  fat_percent.append(num1 * num)\n",
        "  pro_percent.append(num2 * num)\n",
        "  car_percent.append(num3 * num)\n",
        "return calories, fat_percent/total, pro_percent/total, car_percent/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Nw4lNDbS0U"
      },
      "source": [
        "# Data collection\n",
        "\n",
        "1. Testing of frame rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "cLuNA1Z141TE",
        "outputId": "ce5d9e45-1e6d-48ed-a5bf-a20bf6cbd5b3"
      },
      "source": [
        "points = np.size(times)\n",
        "print('Collected points:', points)\n",
        "sorted = np.sort(times)\n",
        "print('Average loop time(s):', np.mean(sorted))\n",
        "print('Standard deviation(s):', np.std(sorted))\n",
        "plt.hist(sorted, density=True, bins=30)\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected points: 4\n",
            "Average loop time(s): 0.04531747102737427\n",
            "Standard deviation(s): 0.001580339296226081\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxDhmweu8Cjs",
        "outputId": "de08a388-1f13-4025-eb76-2d22ee225c73"
      },
      "source": [
        "print(store)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[160.6068081863061, 168.96401334933137, 'up', -1], [160.6068081863061, 168.96401334933137, 'up', -1], [160.6068081863061, 168.96401334933137, 'up', -1]]\n"
          ]
        }
      ]
    }
  ]
}